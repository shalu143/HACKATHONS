from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = 'all'
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler
from sklearn.model_selection import cross_val_score,GridSearchCV,StratifiedKFold,RepeatedStratifiedKFold,RandomizedSearchCV
from sklearn.metrics import accuracy_score,auc,recall_score,precision_score,precision_recall_curve,confusion_matrix,roc_auc_score,f1_score
from xgboost import XGBClassifier
from sklearn.pipeline import make_pipeline
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
#from sklearn.impute import SimpleImputer,KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.impute import MissingIndicator
import re
import seaborn as sns
from matplotlib import pyplot as plt
sns.set(style="darkgrid")

path = "AV hackathon"
train = pd.read_csv(f'{path}/train_for_model.csv',dtype = {'enrollee_id':str})
test = pd.read_csv(f'{path}/test_for_model.csv',dtype = {'enrollee_id':str})


features = train.select_dtypes('number').drop('target', axis = 1).columns
features

X_train, X_test, y_train, y_test = train_test_split(train[features],\
                train.target,test_size = 0.20, random_state = 69, stratify = train.target)

x=XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.6, gamma=5, gpu_id=-1,
              importance_type='gain', interaction_constraints=None,
              learning_rate=0.02, max_delta_step=0, max_depth=5,
              min_child_weight=5, monotone_constraints=None,
              n_estimators=600, n_jobs=-1, num_parallel_tree=1,
              objective='binary:logistic', random_state=0, reg_alpha=0,
              reg_lambda=1, scale_pos_weight=1, silent=True, subsample=1.0,
              tree_method=None, validate_parameters=False, verbosity=None)

x.fit(X_train, y_train)

f'train_roca : {roc_auc_score(y_train,x.predict_proba(X_train)[:,1])} ; test_roca : {roc_auc_score(y_test,x.predict_proba(X_test)[:,1])}'

from datetime import datetime
def timer(start_time=None):
    if not start_time:
        start_time = datetime.now()
        return start_time
    elif start_time:
        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)
        tmin, tsec = divmod(temp_sec, 60)
        print('\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))

params = {
        'min_child_weight': [10,11,12,13,14,15,16,17,18,19],
        'gamma': [ 1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2,2.1,2.2,2.3,2.5,3.1,3.2,3.3,3.4,3.6,3.7,3.8,3.9,4,3.5, 4,5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.4,0.6,0.7,0.5, 0.8,0.9,1.0],
        'max_depth': [1,2,3,4,5,6,7]
        }

xgbm = XGBClassifier(random_state = 69, colsample = 0.5, learning_rate = 0.01,n_estimators =500,\
           scale_pos_weight  = 6.57) 

folds = 5
param_comb = 5

skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 69)

random_search = RandomizedSearchCV(xgbm, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf, verbose=3, random_state=69 )

# Here we go
start_time = timer(None) # timing starts from this point for "start_time" variable
random_search.fit(X_train, y_train)
timer(start_time) 

print('\n All results:')
print(random_search.cv_results_)
print('\n Best estimator:')
print(random_search.best_estimator_)
print('\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))
print(random_search.best_score_ * 2 - 1)
print('\n Best hyperparameters:')
print(random_search.best_params_)

t=XGBClassifier(base_score=0.5, booster=None, colsample=0.5,
       colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,
       gamma=1.5, gpu_id=-1, importance_type='gain',
       interaction_constraints=None, learning_rate=0.01, max_delta_step=0,
       max_depth=3, min_child_weight=11,
       monotone_constraints=None, n_estimators=500, n_jobs=0,
       num_parallel_tree=1, objective='binary:logistic', random_state=69,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=6.57, subsample=1.0,
       tree_method=None, validate_parameters=False, verbosity=None)

t.fit(X_train, y_train)

f'train_roca : {roc_auc_score(y_train,t.predict_proba(X_train)[:,1])} ; test_roca : {roc_auc_score(y_test,t.predict_proba(X_test)[:,1])}'
pred_bin = t.predict(X_train)

f1_score(y_train ,pred_bin)

df_sub = test[['enrollee_id']]
df_sub['target'] = t.predict_proba(test[features])[:,1]
df_sub['target'] = t.predict(test[features])
df_sub.to_csv(f'{path}/my_submission_v6.csv',index = False)
