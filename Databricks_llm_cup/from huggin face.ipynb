{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be6029d-c892-4b62-9589-5d20e7f1aa4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8f161adb304d0b94ade337210ff10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# Login to Huggingface to get access to the model\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c335ac03-48cf-48f8-928f-6de9e06b15e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting xformers\n  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.8/211.8 MB 10.0 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from xformers) (1.21.5)\nCollecting torch==2.1.0\n  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 2.0 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.6 MB/s eta 0:00:00\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch==2.1.0->xformers) (2.8.4)\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 72.2 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 73.3 MB/s eta 0:00:00\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 19.1 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from torch==2.1.0->xformers) (3.6.0)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch==2.1.0->xformers) (2.11.3)\nCollecting triton==2.1.0\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.2/89.2 MB 25.0 MB/s eta 0:00:00\nCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 89.5 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 19.0 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch==2.1.0->xformers) (4.3.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 61.2 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 11.3 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.18.1\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 4.1 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 9.8 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 6.0 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==8.9.2.26\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.3 MB/s eta 0:00:00\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from torch==2.1.0->xformers) (2022.7.1)\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/20.5 MB 28.4 MB/s eta 0:00:00\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch==2.1.0->xformers) (2.0.1)\nCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 5.7 MB/s eta 0:00:00\nInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n  Attempting uninstall: torch\n    Found existing installation: torch 1.13.1+cu117\n    Not uninstalling torch at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-69c543a0-667e-455a-b306-311ac5d9eb3e\n    Can't uninstall 'torch'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.14.1+cu117 requires torch==1.13.1, but you have torch 2.1.0 which is incompatible.\nSuccessfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.1.0 triton-2.1.0 xformers-0.0.22.post7\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb69612-9aca-4ea3-87c3-7d259bd1f6bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c45bfff-f3ad-4747-9f58-61af130946aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b1a120177c48bea69c793b833cab99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb4773c0ee34effbd5b5b99f97ec1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27232e2f8ab5448bbf92ae9ea2a6506c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eae075f8ff44e088b35da903f8545d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63093cf5b5a247738406a7963d103df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d7450235/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a85438b03e41d3ad7385992429aa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ffb722cf894afcbe95eaf380a96c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a655ff5e4647423996aa48e64d6757d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69291b5391ec4a04a4fef9b494d29aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f089d3b14d4c4f2bbf8786d4c7d7c21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ceca5316d641da9311c0304deeca30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# it is suggested to pin the revision commit hash and not change it for reproducibility because the uploader might change the model afterwards; you can find the commmit history of llamav2-7b-chat in https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/commits/main\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "revision = \"08751db2aca9bf2f7f80d2e516117a53d7450235\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=\"left\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    revision=revision,\n",
    "    return_full_text=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442223db-20b2-4232-80f6-26e10915a0dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Define prompt template to get the expected features and performance for the chat versions. See our reference code in github for details: https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"\n",
    "<s>[INST]<<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "[/INST]\n",
    "\"\"\".format(\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    instruction=\"{instruction}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a909eb-ce8d-4abd-9ebe-3df82cf8774f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def gen_text(prompts, use_template=False, **kwargs):\n",
    "    if use_template:\n",
    "        full_prompts = [\n",
    "            PROMPT_FOR_GENERATION_FORMAT.format(instruction=prompt)\n",
    "            for prompt in prompts\n",
    "        ]\n",
    "    else:\n",
    "        full_prompts = prompts\n",
    "\n",
    "    if \"batch_size\" not in kwargs:\n",
    "        kwargs[\"batch_size\"] = 1\n",
    "    \n",
    "    # the default max length is pretty small (20), which would cut the generated output in the middle, so it's necessary to increase the threshold to the complete response\n",
    "    if \"max_new_tokens\" not in kwargs:\n",
    "        kwargs[\"max_new_tokens\"] = 1000\n",
    "\n",
    "    # configure other text generation arguments, see common configurable args here: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
    "    kwargs.update(\n",
    "        {\n",
    "            \"pad_token_id\": tokenizer.eos_token_id,  # Hugging Face sets pad_token_id to eos_token_id by default; setting here to not see redundant message\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    outputs = pipeline(full_prompts, **kwargs)\n",
    "    outputs = [out[0][\"generated_text\"] for out in outputs]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3099cd-6b90-4e34-977d-0be7075312a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\nMasala dosa is a popular South Indian dish that is made with a fermented rice and lentil batter, filled with spiced potatoes and onions, and served with sambar and chutney. Here is a recipe for masala dosa:\n\nIngredients:\n\nFor the dosa batter:\n\n* 1 cup of rice\n* 1 cup of lentils (split red lentils or toor dal)\n* 1/2 cup of water\n* 1/4 teaspoon of salt\n* 1/4 teaspoon of sugar\n* 1/2 teaspoon of active dry yeast\n* 1/4 teaspoon of asafetida (optional)\n\nFor the filling:\n\n* 2 medium-sized potatoes, peeled and grated\n* 1 medium-sized onion, finely chopped\n* 1/4 teaspoon of ground cumin\n* 1/4 teaspoon of ground coriander\n* 1/4 teaspoon of turmeric powder\n* Salt, to taste\n* 2 tablespoons of oil\n\nInstructions:\n\n1. To make the dosa batter, soak the rice and lentils in water for 4-5 hours or overnight. Drain the water and grind the rice and lentils into a smooth batter using a mixer or blender. Add the salt, sugar, yeast, and asafetida (if using) to the batter and mix well. Cover the batter with a cloth and let it ferment for 12-24 hours in a warm place.\n2. To make the filling, heat the oil in a pan and add the grated potatoes and chopped onions. Cook until the potatoes are golden brown and the onions are translucent. Add the cumin, coriander, turmeric, and salt, and mix well.\n3. To make the dosas, heat a non-stick pan or griddle over medium heat. Pour a ladle of batter onto the pan and spread it evenly to form a thin layer. Cook for 1-2 minutes, until the edges start to curl and the surface is dry. Flip the dosa and cook for another 1-2 minutes, until it is crispy and golden brown. Repeat with the remaining batter.\n4. Serve the masala dosas with sambar (a spicy lentil soup) and chutney (a spicy condiment made with vegetables or fruits).\n\nNote: You can adjust the amount of spices in the filling to your taste preferences. You can also add other ingredients like minced garlic or ginger to the filling for extra flavor.\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"what is the recipe for masala dosa?\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d655412-6592-471b-9920-77cbecf7a6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = gen_text([\"\"\"Translate the given text to Hindi langauge\n",
    "                 '''A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding'''\n",
    "\"\"\"])\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d855342-92f0-40db-b6c6-93f44dc5db98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक बड़ी संस्कृति में संचालित हैं एक संस्कृति का प्रारंभिक संचालन हैं, जो एक बड़ा संकलन का संचालन करता हैं, जो एक नाम का संचालन करता हैं, जो संचालन के साथ संचालित हैं, जो संचालन के साथ संच\n"
     ]
    }
   ],
   "source": [
    "results = gen_text([\"\"\"change the given text to Hindi langauge\n",
    "                 '''A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding'''\n",
    "\"\"\"], temperature=0.1, max_new_tokens=200, use_template=True)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf1aac1-67eb-4b8d-82a7-b4ef118d581b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTranslation in Hindi:\n'''एक बड़ा भाषा मॉडल एक प्रकार का इंटेलिजेंस मॉडल है | इसे बड़ा संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का संकलन का\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b4fda8-994c-40db-94df-398d1610968e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens for input: 777\nHannah Arendt was a prominent political thinker of the 20th century, known for her works such as \"The Origins of Totalitarianism\" and \"The Human Condition.\" Her political philosophy is distinct from traditional conservatism, liberalism, and socialism, and cannot be classified into the recent revival of communitarian thought. While some critics have invoked her name as a critique of liberal tradition, Arendt was actually a defender of constitutionalism and the rule of law, advocating for fundamental human rights and civic engagement. Her conception of politics is based on active citizenship and collective deliberation, rather than individual preferences or integration around a shared conception of the\n"
     ]
    }
   ],
   "source": [
    "long_input = \"\"\"Provide a concise summary of the below passage.\n",
    "\n",
    "Hannah Arendt was one of the seminal political thinkers of the twentieth century. The power and originality of her thinking was evident in works such as The Origins of Totalitarianism, The Human Condition, On Revolution and The Life of the Mind. In these works and in numerous essays she grappled with the most crucial political events of her time, trying to grasp their meaning and historical import, and showing how they affected our categories of moral and political judgment. What was required, in her view, was a new framework that could enable us to come to terms with the twin horrors of the twentieth century, Nazism and Stalinism. She provided such framework in her book on totalitarianism, and went on to develop a new set of philosophical categories that could illuminate the human condition and provide a fresh perspective on the nature of political life.\n",
    "\n",
    "Although some of her works now belong to the classics of the Western tradition of political thought, she has always remained difficult to classify. Her political philosophy cannot be characterized in terms of the traditional categories of conservatism, liberalism, and socialism. Nor can her thinking be assimilated to the recent revival of communitarian political thought, to be found, for example, in the writings of A. MacIntyre, M. Sandel, C. Taylor and M. Walzer. Her name has been invoked by a number of critics of the liberal tradition, on the grounds that she presented a vision of politics that stood in opposition some key liberal principles. There are many strands of Arendt’s thought that could justify such a claim, in particular, her critique of representative democracy, her stress on civic engagement and political deliberation, her separation of morality from politics, and her praise of the revolutionary tradition. However, it would be a mistake to view Arendt as an anti-liberal thinker. Arendt was in fact a stern defender of constitutionalism and the rule of law, an advocate of fundamental human rights (among which she included not only the right to life, liberty, and freedom of expression, but also the right to action and to opinion), and a critic of all forms of political community based on traditional ties and customs, as well as those based on religious, ethnic, or racial identity.\n",
    "\n",
    "Arendt’s political thought cannot, in this sense, be identified either with the liberal tradition or with the claims advanced by a number of its critics. Arendt did not conceive of politics as a means for the satisfaction of individual preferences, nor as a way to integrate individuals around a shared conception of the good. Her conception of politics is based instead on the idea of active citizenship, that is, on the value and importance of civic engagement and collective deliberation about all matters affecting the political community. If there is a tradition of thought with which Arendt can be identified, it is the classical tradition of civic republicanism originating in Aristotle and embodied in the writings of Machiavelli, Montesquieu, Jefferson, and Tocqueville. According to this tradition politics finds its authentic expression whenever citizens gather together in a public space to deliberate and decide about matters of collective concern. Political activity is valued not because it may lead to agreement or to a shared conception of the good, but because it enables each citizen to exercise his or her powers of agency, to develop the capacities for judgment and to attain by concerted action some measure of political efficacy.\"\"\"\n",
    "\n",
    "def get_num_tokens(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    return inputs.shape[1]\n",
    "\n",
    "print('number of tokens for input:', get_num_tokens(long_input))\n",
    "\n",
    "results = gen_text([long_input], max_new_tokens=150, use_template=True)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "243601fc-1a9b-4708-9c7b-a1a5738e14d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens for input: 52\nएक बड़ी भाषा मॉडल एक प्रकार के संज्ञानुसार काउंट्रीबुक एक बड़ा संकलन का प्रारंभ करता है, जहां एक बड़ा संकलन का प्रारंभ करता है, जहां \n"
     ]
    }
   ],
   "source": [
    "long_input = \"\"\"Translate the given text to Hindi langauge\n",
    "                 A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding\n",
    "\"\"\"\n",
    "\n",
    "def get_num_tokens(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    return inputs.shape[1]\n",
    "\n",
    "print('number of tokens for input:', get_num_tokens(long_input))\n",
    "\n",
    "results = gen_text([long_input], max_new_tokens=150, use_template=True)\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e8d3c61-d490-4e36-9a60-c0ab6eb8ec8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "from huggin face",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
